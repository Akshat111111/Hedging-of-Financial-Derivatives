{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "95a5bfd638c9824676f5f8cc480e7f12c07555a6"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mxgboost\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mlgb\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV, RandomizedSearchCV\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_scorer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import re\n",
    "from scipy import stats\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 5)\n",
    "matplotlib.rcParams['font.size'] = 12\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "import time\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import get_scorer\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "import lightgbm as lgb\n",
    "from sklearn.externals.joblib import Parallel, delayed\n",
    "from sklearn.base import clone\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lightgbm\n",
      "  Using cached lightgbm-4.3.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/shikarichacha/anaconda3/lib/python3.11/site-packages (from lightgbm) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/shikarichacha/anaconda3/lib/python3.11/site-packages (from lightgbm) (1.11.1)\n",
      "Building wheels for collected packages: lightgbm\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mBuilding wheel for lightgbm \u001b[0m\u001b[1;32m(\u001b[0m\u001b[32mpyproject.toml\u001b[0m\u001b[1;32m)\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[41 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,255 - scikit_build_core - INFO - RUN: /private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-build-env-o6_lwrml/normal/lib/python3.11/site-packages/cmake/data/bin/cmake -E capabilities\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,261 - scikit_build_core - INFO - CMake version: 3.29.3\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1m\u001b[92mscikit-build-core 0.9.5\u001b[0m using \u001b[94mCMake 3.29.3\u001b[0m \u001b[91m(wheel)\u001b[0m\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,264 - scikit_build_core - INFO - Build directory: /private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mConfiguring CMake...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,272 - scikit_build_core - INFO - RUN: /private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-build-env-o6_lwrml/normal/lib/python3.11/site-packages/ninja/data/bin/ninja --version\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,473 - scikit_build_core - INFO - Ninja version: 1.11.1\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,475 - scikit_build_core - WARNING - Can't find a Python library, got libdir=/Users/shikarichacha/anaconda3/lib, ldlibrary=libpython3.11.a, multiarch=darwin, masd=None\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:19,477 - scikit_build_core - INFO - RUN: /private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-build-env-o6_lwrml/normal/lib/python3.11/site-packages/cmake/data/bin/cmake -S. -B/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build -DCMAKE_BUILD_TYPE:STRING=Release -C/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build/CMakeInit.txt -DCMAKE_MAKE_PROGRAM=/private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-build-env-o6_lwrml/normal/lib/python3.11/site-packages/ninja/data/bin/ninja -D__BUILD_FOR_PYTHON:BOOL=ON\n",
      "  \u001b[31m   \u001b[0m loading initial cache file /var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build/CMakeInit.txt\n",
      "  \u001b[31m   \u001b[0m -- The C compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- The CXX compiler identification is AppleClang 15.0.0.15000309\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working C compiler: /Library/Developer/CommandLineTools/usr/bin/cc - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting C compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compiler ABI info - done\n",
      "  \u001b[31m   \u001b[0m -- Check for working CXX compiler: /Library/Developer/CommandLineTools/usr/bin/c++ - skipped\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features\n",
      "  \u001b[31m   \u001b[0m -- Detecting CXX compile features - done\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_C (missing: OpenMP_C_FLAGS OpenMP_C_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP_CXX (missing: OpenMP_CXX_FLAGS OpenMP_CXX_LIB_NAMES)\n",
      "  \u001b[31m   \u001b[0m -- Could NOT find OpenMP (missing: OpenMP_C_FOUND OpenMP_CXX_FOUND)\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_C: -Xpreprocessor -fopenmp -I/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP_CXX: -Xpreprocessor -fopenmp -I/include\n",
      "  \u001b[31m   \u001b[0m -- Found OpenMP: TRUE\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_PREFETCH - Failed\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC\n",
      "  \u001b[31m   \u001b[0m -- Performing Test MM_MALLOC - Success\n",
      "  \u001b[31m   \u001b[0m -- Using _mm_malloc\n",
      "  \u001b[31m   \u001b[0m -- Configuring done (1.0s)\n",
      "  \u001b[31m   \u001b[0m -- Generating done (0.0s)\n",
      "  \u001b[31m   \u001b[0m -- Build files have been written to: /var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build\n",
      "  \u001b[31m   \u001b[0m \u001b[92m***\u001b[0m \u001b[1mBuilding project with \u001b[94mNinja\u001b[0m...\u001b[0m\n",
      "  \u001b[31m   \u001b[0m 2024-06-03 12:42:20,537 - scikit_build_core - INFO - RUN: /private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-build-env-o6_lwrml/normal/lib/python3.11/site-packages/cmake/data/bin/cmake --build /var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/tmphztq8zy3/build\n",
      "  \u001b[31m   \u001b[0m ninja: error: '/lib/libomp.dylib', needed by '/private/var/folders/qh/215l544s06x3_jp987f9gdgh0000gn/T/pip-install-qisg1lw1/lightgbm_033a9e90853f4e019eb0ad2e2b184af0/lib_lightgbm.so', missing and no known rule to make it\n",
      "  \u001b[31m   \u001b[0m \n",
      "  \u001b[31m   \u001b[0m \u001b[91m\u001b[1m*** CMake build failed\u001b[0m\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for lightgbm\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25hFailed to build lightgbm\n",
      "\u001b[31mERROR: Could not build wheels for lightgbm, which is required to install pyproject.toml-based projects\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Obtaining dependency information for xgboost from https://files.pythonhosted.org/packages/03/e6/4aef6799badc2693548559bad5b56d56cfe89eada337c815fdfe92175250/xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl.metadata\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy in /Users/shikarichacha/anaconda3/lib/python3.11/site-packages (from xgboost) (1.24.3)\n",
      "Requirement already satisfied: scipy in /Users/shikarichacha/anaconda3/lib/python3.11/site-packages (from xgboost) (1.11.1)\n",
      "Downloading xgboost-2.0.3-py3-none-macosx_12_0_arm64.whl (1.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c20d167ff505b4e4e713539167f0afca20a3e70a"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "43bfe6d1b50990d3a89d4b99f40f599b052d4940"
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (10, 5)\n",
    "matplotlib.rcParams['font.size'] = 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_kg_hide-input": true,
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "from kaggle.competitions import twosigmanews\n",
    "# You can only call make_env() once, so don't lose it!\n",
    "env = twosigmanews.make_env()\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "884991f41f1b46c1484fb5b0e075d9705ac7a089"
   },
   "source": [
    "<a id='load_data'></a>\n",
    "\n",
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3159e3be827a2d2b71aa3e7239db721b2a4d46bf"
   },
   "outputs": [],
   "source": [
    "(market_train_orig, news_train_orig) = env.get_training_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "605e7b8dfb8c6f724234ed0c58471fd58f87ed10"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_orig.copy()\n",
    "news_train_df = news_train_orig.copy()\n",
    "print('Market train shape: ',market_train_df.shape)\n",
    "print('News train shape: ', news_train_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1b0be967abcc7910cda479933c3695e049df35b9"
   },
   "outputs": [],
   "source": [
    "market_train_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7a1a2360a6addb11ee4b70c660f7855a9de59461"
   },
   "outputs": [],
   "source": [
    "news_train_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "93d03d81e1e53857783cd4d650591ea5f0f70485"
   },
   "source": [
    "<a id='explore_news'></a>\n",
    "\n",
    "## Explore news data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "421dc99883a5bfbc3505253d24117b14bb60fff3"
   },
   "source": [
    "### Evolutions over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "aa5720aecade6fd1468c573f8cd70ec95c6798ea"
   },
   "outputs": [],
   "source": [
    "# Sort values by time then extract date\n",
    "news_train_df = news_train_df.sort_values(by='time')\n",
    "news_train_df['date'] = news_train_df['time'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dd1bfcfdd7ee354335e3c109d9964af3e36029c4"
   },
   "outputs": [],
   "source": [
    "# Function to plot time series data\n",
    "def plot_vs_time(data_frame, column, calculation='mean', span=10):\n",
    "    if calculation == 'mean':\n",
    "        group_temp = data_frame.groupby('date')[column].mean().reset_index()\n",
    "    if calculation == 'count':\n",
    "        group_temp = data_frame.groupby('date')[column].count().reset_index()\n",
    "    if calculation == 'nunique':\n",
    "        group_temp = data_frame.groupby('date')[column].nunique().reset_index()\n",
    "    group_temp = group_temp.ewm(span=span).mean()\n",
    "    fig = plt.figure(figsize=(10,3))\n",
    "    plt.plot(group_temp['date'], group_temp[column])\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel(column)\n",
    "    plt.title('%s versus time' %column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e3c874f8a57287261eb8802e443757fee60b4f25"
   },
   "outputs": [],
   "source": [
    "plot_vs_time(news_train_df, 'sourceId', calculation='count', span=10)\n",
    "plt.title('News count vs time')\n",
    "plt.ylabel('Count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b7fcc9200c1ea093ecd5ed5f765948fbd493e0da"
   },
   "outputs": [],
   "source": [
    "# Plot time evolution of several parameters\n",
    "\n",
    "columns = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n",
    "           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H', 'volumeCounts24H']\n",
    "\n",
    "for column in columns:\n",
    "    plot_vs_time(news_train_df, column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2a7dd31ea83d826e30983f23d1496edfe8e6129f"
   },
   "source": [
    "### Time delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "7fecd445017f16c29562d5e0be4c76b7c636bbb6"
   },
   "outputs": [],
   "source": [
    "time_delay = (pd.to_datetime(news_train_df['time']) - pd.to_datetime(news_train_df['firstCreated']))\n",
    "time_delay_log10 = np.log10(time_delay.dt.total_seconds()/60+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "178c55410a14e55423621f658c4fde62707ad5cf"
   },
   "outputs": [],
   "source": [
    "plt.hist(time_delay_log10, bins=np.arange(0,2.5,0.25), rwidth=0.7)\n",
    "plt.xlabel('$Log_{10}$(Time delay in minutes +1)')\n",
    "plt.ylabel('Counts')\n",
    "plt.title('Delay time distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0c008e703fcfdaa8e9b2d0cbe02d34309059f8b0"
   },
   "outputs": [],
   "source": [
    "time_delay_min = time_delay.dt.total_seconds()/60\n",
    "time_delay_df = time_delay_min.to_frame().join(news_train_df['date'].to_frame())\n",
    "time_delay_df.columns = ['delay','date']\n",
    "plot_vs_time(time_delay_df, 'delay')\n",
    "plt.ylabel('Delay (minutes)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b3f483af18a84536e73eb5865e44e33e40cc4979"
   },
   "source": [
    "### Urgency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "dfd02bbc0d41b4d73e46cc7e1b8700089dae0ea8"
   },
   "outputs": [],
   "source": [
    "urgency_count = news_train_df.groupby('urgency')['sourceId'].count()\n",
    "urgency_count = urgency_count/urgency_count.sum()\n",
    "print('Urgency ratio')\n",
    "urgency_count.sort_values(ascending=True)\n",
    "del urgency_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6c154f10f3cae08696309a308ca4d242923c7854"
   },
   "source": [
    "### Take sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2a7510b351cf92b37acb36dcb5c817b592eaf376"
   },
   "outputs": [],
   "source": [
    "take_sequence = news_train_df.groupby('takeSequence')['sourceId'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ee333829bbf07b15e54ba87ec8df7133dd2d8572"
   },
   "outputs": [],
   "source": [
    "take_sequence = take_sequence.sort_values(ascending= False)\n",
    "take_sequence[:10].plot.barh()\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Take sequence')\n",
    "plt.title('Top 10 take sequence')\n",
    "plt.gca().invert_yaxis()\n",
    "del take_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c639ddc777bcb76a0dc2b6868805f452dbc09b04"
   },
   "source": [
    "### Providers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0bb4a59d430e52a2bc0bad214882004ba74ae9ac"
   },
   "outputs": [],
   "source": [
    "provider_count = news_train_df.groupby('provider')['sourceId'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8afbc00a56f039e57b0323b507d68a50c0945fc7"
   },
   "outputs": [],
   "source": [
    "provider_sort = provider_count.sort_values(ascending= False)\n",
    "provider_sort[:10].plot.barh()\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Provider')\n",
    "plt.title('Top 10 news provider')\n",
    "plt.gca().invert_yaxis()\n",
    "del provider_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "268c844df3f359a9faf6b3f877831a0fc156ca3f"
   },
   "source": [
    "### Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "affec3723b6ea7a5a6af4f6e335a4408b696ac54"
   },
   "outputs": [],
   "source": [
    "# Extract data from a single cell\n",
    "def contents_to_list(contents):\n",
    "    text = contents[1:-1]\n",
    "    text = re.sub(r\",\",' ',text)\n",
    "    text = re.sub(r\"'\",\"\", text)\n",
    "    text_list = text.split('  ')\n",
    "    return text_list\n",
    "\n",
    "# Put data from columns into dict\n",
    "def get_content_dict(content_column):\n",
    "    content_dict = {}\n",
    "    for i in range(len(content_column)):\n",
    "        this_cell = content_column[i]\n",
    "        content_list = contents_to_list(this_cell)        \n",
    "        for content in content_list:\n",
    "            if content in content_dict.keys():\n",
    "                content_dict[content] += 1\n",
    "            else:\n",
    "                content_dict[content] = 1\n",
    "    return content_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8b2dd8627043d510263827e170169e25d369ecf3"
   },
   "outputs": [],
   "source": [
    "subjects = news_train_df.sample(n=10000, random_state=1)['subjects']\n",
    "subjects_dict = get_content_dict(subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "2fd0403ff406e0835c7ec7579439c0384b0739fa"
   },
   "outputs": [],
   "source": [
    "subjects_df = pd.Series(subjects_dict).sort_values(ascending=False)\n",
    "subjects_df[:15].plot.barh()\n",
    "plt.ylabel('Subjects')\n",
    "plt.xlabel('Counts')\n",
    "plt.title('Top subjects for 10k data')\n",
    "plt.gca().invert_yaxis()\n",
    "del subjects_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c88ed3ad8727491f21fb888a0683296f2d4eb2ad"
   },
   "source": [
    "### Audiences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1c133b53599e27a513899c5ee88df5586e14af4d"
   },
   "outputs": [],
   "source": [
    "audiences = news_train_df.sample(n=10000, random_state=1)['audiences']\n",
    "audiences_dict = get_content_dict(audiences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9b320b3c7cefc4bf4faa75a06659bf7453aa4751"
   },
   "outputs": [],
   "source": [
    "audiences_df = pd.Series(audiences_dict).sort_values(ascending=False)\n",
    "audiences_df[:15].plot.barh()\n",
    "plt.ylabel('Audiences')\n",
    "plt.xlabel('Counts')\n",
    "plt.title('Top audiences for 10k data')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "09df65719e6b49c9aed14eb4bacf887c8c91eac5"
   },
   "source": [
    "### Company Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ef4fefa4b51a8fce038ec628d9d7025ed377fc39"
   },
   "outputs": [],
   "source": [
    "news_train_df['companyCount'].hist(bins=np.arange(0,30,1))\n",
    "plt.xlabel('Company count')\n",
    "plt.title('Company count distribution')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "23d56742da7c14f5df9db324c3716a06bc56a10e"
   },
   "source": [
    "### Head line tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3d03d5ab32732285e3b0ee07232289089c2ae1f2"
   },
   "outputs": [],
   "source": [
    "head_line = news_train_df.groupby('headlineTag')['sourceId'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "74d3b36c58ddb2df46bcf1d63e724a0afb0446dd"
   },
   "outputs": [],
   "source": [
    "head_line_sort = head_line.sort_values(ascending= False)\n",
    "head_line_sort[:10].plot.barh()\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Head line')\n",
    "plt.title('Top 10 head lines')\n",
    "plt.gca().invert_yaxis()\n",
    "del head_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5282c1098f638173a19b2baf6ae962f367b3da55"
   },
   "source": [
    "Most headlines are blank. This properties may not be important."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6d393bbd554a89347a8bdeedd41b936499a5fdcb"
   },
   "source": [
    "### First sentence - Urgency - relevance - sentiment Word Count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6598e6f49f93908f5ed202a5a224a455f5c41f67"
   },
   "source": [
    "**First sentence and urgency**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "72ebe9b92e4338720abad56321c93b5a2f44f631"
   },
   "outputs": [],
   "source": [
    "news_train_df['firstMentionSentence'].hist(bins=np.arange(0,20,1))\n",
    "plt.xlabel('First mention sentence')\n",
    "plt.ylabel('Count')\n",
    "plt.title('First mention sentence distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b91e025e67d3020b121b0703f962584afb6f21de"
   },
   "outputs": [],
   "source": [
    "sentence_urgency = news_train_df.groupby('firstMentionSentence')['urgency'].mean()\n",
    "sentence_urgency.head(5)\n",
    "del sentence_urgency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "2f69a8170e80326efbff05f83f25c0cc92763356"
   },
   "source": [
    "**First sentence and relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "46f38eb2f0b5138ba5a5a6113b8d9d0f091eea80"
   },
   "outputs": [],
   "source": [
    "news_train_df['relevance'].hist(bins=np.arange(0,1.01,0.05))\n",
    "plt.xlabel('Relevance')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Relevance distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "31b32fce26d0450ca58fb4db606ee2725b31d4ec"
   },
   "outputs": [],
   "source": [
    "sentence_relevance = news_train_df.groupby('firstMentionSentence')['relevance'].mean()\n",
    "sentence_relevance[:15].plot.barh()\n",
    "plt.xlabel('Relevance')\n",
    "plt.title('Relevance by sentence')\n",
    "plt.gca().invert_yaxis()\n",
    "del sentence_relevance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4b956b17b8d8936c378c16ba717f036dac2a084"
   },
   "source": [
    "**Sentiment word count and relevance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "28bfb6c469d18be39e58f4812e2a2c5077d3de71"
   },
   "outputs": [],
   "source": [
    "sentimentWordCount = news_train_df.groupby('sentimentWordCount')['sourceId'].count().reset_index()\n",
    "plt.plot(sentimentWordCount['sentimentWordCount'], sentimentWordCount['sourceId'])\n",
    "plt.xlim(0,300)\n",
    "plt.xlabel('Sentiment words count')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment words count distribution')\n",
    "del sentimentWordCount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4e522d43517a990c3d8392d37d823febd5404337"
   },
   "outputs": [],
   "source": [
    "sentimentWordRatio = news_train_df.groupby('sentimentWordCount')['relevance'].mean()\n",
    "plt.plot(sentimentWordRatio)\n",
    "plt.xlim(0,2000)\n",
    "plt.ylabel('Relevance')\n",
    "plt.xlabel('Sentiment word count')\n",
    "plt.title('Sentiment word count and relevance')\n",
    "del sentimentWordRatio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "878f90fb7f2855f0061c3babfcbc481a9e665254"
   },
   "source": [
    "**Sentiment ratio**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "38051d1570778ad1f6f2f43685aefa3f0f9c7d90"
   },
   "outputs": [],
   "source": [
    "news_train_df['sentimentRatio'] = news_train_df['sentimentWordCount']/news_train_df['wordCount']\n",
    "news_train_df['sentimentRatio'].hist(bins=np.linspace(0,1.001,40))\n",
    "plt.xlabel('Sentiment ratio')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Sentiment ratio distribution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "a46cf2ec80d7459b562fdd09135106eb561e99db"
   },
   "outputs": [],
   "source": [
    "news_train_df.sample(n=10000, random_state=1).plot.scatter('sentimentRatio', 'relevance')\n",
    "plt.title('Relevance vs sentiment ratio of 10k samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5337827b2023bf3bb3f5cfcd98eccaa88874aef6"
   },
   "source": [
    "### Asset name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "26cb4dd8efd59802e83ac1e0f63eff19a04809b6"
   },
   "outputs": [],
   "source": [
    "asset_name = news_train_df.groupby('assetName')['sourceId'].count()\n",
    "print('Total number of assets: ',news_train_df['assetName'].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c58a857268ced6707550d79b894098759300d021"
   },
   "outputs": [],
   "source": [
    "asset_name = asset_name.sort_values(ascending=False)\n",
    "asset_name[:10].plot.barh()\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel('Count')\n",
    "plt.title('Top 10 assets news')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b21a188d06dc8abdef25fec298632a7a43858f32"
   },
   "outputs": [],
   "source": [
    "for i, j in zip([-1, 0, 1], ['negative', 'neutral', 'positive']):\n",
    "    df_sentiment = news_train_df.loc[news_train_df['sentimentClass'] == i, 'assetName']\n",
    "    print(f'Top mentioned companies for {j} sentiment are:')\n",
    "    print(df_sentiment.value_counts().head(5))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a79869fa1cf3ea6b19b1c7187f95c66ad49a8e5e"
   },
   "source": [
    "### Remove outliers and plot correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": false,
    "_uuid": "61f5cac4814e81fc5c600cf2ef90032a570764f6"
   },
   "outputs": [],
   "source": [
    "# Function to remove outliers\n",
    "def remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n",
    "    temp_frame = data_frame\n",
    "    for column in column_list:\n",
    "        this_column = data_frame[column]\n",
    "        quant_df = this_column.quantile([low,high])\n",
    "        low_limit = quant_df[low]\n",
    "        high_limit = quant_df[high]\n",
    "        temp_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n",
    "    return temp_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "519a776b4a1fdbe32c60fb3644d6542162c26e1f"
   },
   "outputs": [],
   "source": [
    "# Remove outlier\n",
    "columns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n",
    "                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n",
    "                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\n",
    "news_rmv_outlier = remove_outliers(news_train_df, columns_outlier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9f07e684c3bf01462a34db96df8bdb262aeb7390"
   },
   "outputs": [],
   "source": [
    "# Plot correlation\n",
    "columns_corr = ['urgency', 'takeSequence', 'companyCount','marketCommentary','sentenceCount',\\\n",
    "           'firstMentionSentence','relevance','sentimentClass','sentimentWordCount','noveltyCount24H',\\\n",
    "           'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D','volumeCounts24H','volumeCounts3D','volumeCounts5D','volumeCounts7D']\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(18,15))\n",
    "sns.heatmap(news_rmv_outlier[columns_corr].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\n",
    "plt.title('Pair-wise correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0b3c24baa4f48418432e25222af4842e798bf168"
   },
   "source": [
    "<a id='explore_market'></a>\n",
    "\n",
    "## Explore market data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "668174b0fcce4d2c3790a349d514daa7bca1226a"
   },
   "outputs": [],
   "source": [
    "print('Check null data:')\n",
    "market_train_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "16557cc1e355af4ae50c43c0028c74845345b9f9"
   },
   "source": [
    "**Some preprocessing:**\n",
    "* Sort data in chronological order\n",
    "* All NAN data comes from the market adjusted column. We fill them up with the raw value data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ddd1a118e6bba8a47edde4f72ade8a816413ae7b"
   },
   "outputs": [],
   "source": [
    "# Sort data\n",
    "market_train_df = market_train_df.sort_values('time')\n",
    "market_train_df['date'] = market_train_df['time'].dt.date\n",
    "\n",
    "# Fill nan\n",
    "market_train_fill = market_train_df\n",
    "column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n",
    "for i in range(len(column_raw)):\n",
    "    market_train_fill[column_market[i]] = market_train_fill[column_market[i]].fillna(market_train_fill[column_raw[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "39183944170d9622d94e395f47e9513137dc1fd8"
   },
   "source": [
    "### Plot data versus time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8f38c5b5e7cb8dfda222be19a3c31e5b81f2911d"
   },
   "outputs": [],
   "source": [
    "plot_vs_time(market_train_fill, 'assetCode', 'count')\n",
    "plt.title('Number of asset codes versus time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6b7cf7be47bba2c1365e37736af026c45aac7d83"
   },
   "outputs": [],
   "source": [
    "# Inspired by https://www.kaggle.com/artgor/eda-feature-engineering-and-everything\n",
    "for i in [0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['close'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['close'], label='%.2f quantile' %i)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Price')\n",
    "plt.title('Market close price by quantile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b91a7e0dea0e737138e45cd04fa12d84a2c0e5f2"
   },
   "outputs": [],
   "source": [
    "for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['returnsClosePrevRaw1'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['returnsClosePrevRaw1'], label='%.2f quantile' %i)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('returnsClosePrevRaw1 by quantile')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bb4fda0922c1972985f2d81647163da8a34b610f"
   },
   "outputs": [],
   "source": [
    "for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['returnsOpenPrevRaw10'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['returnsOpenPrevRaw10'], label='%.2f quantile' %i)\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('returnsOpenPrevRaw10 by quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "d4f4f4fa8b458c4f50ff27def35dc24540d6127f"
   },
   "outputs": [],
   "source": [
    "for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['returnsOpenPrevMktres10'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['returnsOpenPrevMktres10'], label='%.2f quantile' %i)\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('returnsOpenPrevMktres10 by quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "df5a76133257e46899c6e07ff09a5628707dcfb9"
   },
   "outputs": [],
   "source": [
    "for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['returnsOpenNextMktres10'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['returnsOpenNextMktres10'], label='%.2f quantile' %i)\n",
    "plt.legend(loc=1)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('returnsOpenNextMktres10 by quantiles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "aeb61e036a694497c9212da7a67b07ec98ba6889"
   },
   "outputs": [],
   "source": [
    "for i in [0.05, 0.25, 0.5, 0.75, 0.95]:\n",
    "    price_df = market_train_fill.groupby('date')['volume'].quantile(i).reset_index()\n",
    "    plt.plot(price_df['date'], price_df['volume'], label='%.2f quantile' %i)\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Volumes')\n",
    "plt.title('Market trade volumes by quantile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "34dfa930af54b34f893f2f18edd7fe3a3c0ac1cd"
   },
   "source": [
    "### Difference between raw values and market adjusted values\n",
    "\n",
    "Let see if there's any difference between raw return and market adjusted return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4600b75bb23819235e35ca1bfe35b18b81321a22"
   },
   "outputs": [],
   "source": [
    "column_mkt_raw_diff = []\n",
    "for i in range(len(column_market)):\n",
    "    this_raw = column_raw[i]\n",
    "    this_market = column_market[i]\n",
    "    new_column_name = 'mkt_raw_diff'+this_raw.replace('returns','').replace('Raw','')\n",
    "    column_mkt_raw_diff.append(new_column_name)\n",
    "    market_train_fill[new_column_name] = market_train_fill[this_market] - market_train_fill[this_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "87f6a951ac0ce9cc805c1912132aaafdf19dcac2"
   },
   "outputs": [],
   "source": [
    "market_train_fill[column_mkt_raw_diff].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05091d2a6d8b5e2bd0e717180dd7e8e5eb6822c0"
   },
   "source": [
    "The difference between raw return and market adjusted returns are negligible, but there are some extreme values. Those values are noise and needs to be taken care of"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "fa11197b3cf981e5a4c7e3c8c2e79e1cb9e58b81"
   },
   "source": [
    "### Asset codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "07f4184a828b14d81c81d0a04b6a517ea9672024"
   },
   "outputs": [],
   "source": [
    "assetCode_df = market_train_df.groupby('assetCode')['volume'].sum().sort_values(ascending=False)\n",
    "print('There are %i unique asset code' %len(assetCode_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3fe3e9743c61553af8cf70a72354f228d8b1ea7d"
   },
   "outputs": [],
   "source": [
    "unknown_name = market_train_fill[market_train_fill['assetName']=='Unknown']\n",
    "unknown_count = unknown_name['assetCode'].value_counts().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cd8c0bb5582d334fcde7fda425693006dd78cacc"
   },
   "outputs": [],
   "source": [
    "print('There are %i unique asset code with unknown asset name' %len(unknown_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "54d8013519b3c23df5806c19e133c26027fc30f2"
   },
   "outputs": [],
   "source": [
    "unknown_count[:15].plot.barh()\n",
    "plt.ylabel('assetCode')\n",
    "plt.xlabel('Counts')\n",
    "plt.title('Top 15 asset code with Unknown asset name')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e44ac81a3a1f9eeba184befc5556c85428c79d7a"
   },
   "outputs": [],
   "source": [
    "assetCode_df[:15].plot.barh()\n",
    "plt.ylabel('assetCode')\n",
    "plt.xlabel('Trading volume')\n",
    "plt.title('Top 15 asset code by volume')\n",
    "plt.gca().invert_yaxis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e47eb7a4665a0fac13785ead0a45499fe56a156d"
   },
   "source": [
    "### Asset Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f135c52c9f11550d8e7efc8453b0d0599568261f"
   },
   "outputs": [],
   "source": [
    "assetName_Volume = market_train_df.groupby('assetName')['volume'].sum().sort_values(ascending=False)\n",
    "assetName_Volume[:15].plot.barh()\n",
    "plt.ylabel('assetName')\n",
    "plt.xlabel('Trading volume')\n",
    "plt.title('Top 15 asset name by volume')\n",
    "plt.gca().invert_yaxis()\n",
    "del assetName_Volume"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "76a0bd4527ed859af58a14c622498dae7e569589"
   },
   "source": [
    "The volume ranking by coorperation seems to be the same as the rank of asset codes they own, e.g. the one with most popular codes has the most trading volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1c6311b4d3bc2b4936f67a6c2a923745369cfaa5"
   },
   "outputs": [],
   "source": [
    "assetName_code = market_train_df.groupby('assetName')['assetCode'].nunique().reset_index().sort_values(by='assetCode',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "58e5eab7313a6c761a966d50afd346a88d9dd183"
   },
   "outputs": [],
   "source": [
    "assetCodeCount = assetName_code.groupby('assetCode')['assetName'].count().reset_index()\n",
    "assetCodeCount.columns = ['assetCodeNo', 'counts']\n",
    "assetCodeCount.head()\n",
    "del assetCodeCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "05dcf2dcf487b67c7f46072e6522cef27462cba9"
   },
   "source": [
    "**The vast majority of companies has only one asset code**. One '*company*' that has 110 actually is the  'Unknown' category. Magically, some companies don't even have any asset code. Currently I have no explanation for this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "a998673ff52c0ad258545f3463a5dda4180a1973"
   },
   "source": [
    "### Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "c6b2efee6c2a29459f43d2157d284692ec42f6f1"
   },
   "outputs": [],
   "source": [
    "columns_corr_market = ['volume', 'open', 'close','returnsClosePrevRaw1','returnsOpenPrevRaw1',\\\n",
    "           'returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10','returnsOpenPrevRaw10',\\\n",
    "           'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'returnsOpenNextMktres10']\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(18,15))\n",
    "sns.heatmap(market_train_fill[columns_corr_market].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\n",
    "plt.title('Pair-wise correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "9b34d7c7c080b802ad0dafb6da7a3df5d3874368"
   },
   "source": [
    "### Dig deeper to a single asset\n",
    "\n",
    "Let's take a closer look to a single asset. Here I choose the one with largest trading volumen: 'Bank of America Corp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "32a49ef70eb5a6f655ee7c431830b5fef9be3037"
   },
   "outputs": [],
   "source": [
    "assetCode = 'Bank of America Corp'\n",
    "thisAssetMark_df = market_train_fill[market_train_fill['assetName']==assetCode].sort_values(by='date',ascending=True) \n",
    "thisAssetMark_df['diff_open_close'] = thisAssetMark_df['open'] - thisAssetMark_df['close']\n",
    "thisAssetNews_df = news_rmv_outlier[news_rmv_outlier['assetName']==assetCode]\n",
    "# Trading volume vs time\n",
    "thisAssetMark_df.plot(x='date', y='volume')\n",
    "plt.title('Trading volume vs time')\n",
    "# Price vs time\n",
    "thisAssetMark_df.plot(x='date', y='open')\n",
    "plt.title('Open price vs time')\n",
    "# Return vs time\n",
    "thisAssetMark_df.plot(x='date', y=['returnsOpenPrevRaw1', 'returnsOpenPrevRaw10','returnsOpenNextMktres10'], alpha=0.8)\n",
    "plt.title('Return vs time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "cb8bb686bfbfdee120cba293ee49aa03ef005205"
   },
   "source": [
    "It can be seen that trading volume is strongly associated with price, i.e. trade increase when price hits bottom. Return is also strongly fluctuated at such time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ebdcf1bf67977eac53266a8bcdc68dfab3a8d073"
   },
   "outputs": [],
   "source": [
    "news_volume = thisAssetNews_df.groupby('date')['sourceId'].count().reset_index()\n",
    "news_volume = news_volume.ewm(span=10).mean()\n",
    "news_volume.plot(x='date',y='sourceId')\n",
    "plt.title('News volume vs time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "75dae3d3fa018e6540e46f31dfafdc2a63bca097"
   },
   "outputs": [],
   "source": [
    "news_urgency = thisAssetNews_df.groupby('date')['urgency'].mean().reset_index()\n",
    "news_urgency = news_urgency.ewm(span=10).mean()\n",
    "news_urgency.plot(x='date',y='urgency')\n",
    "plt.title('News urgency vs time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "af4cc30681e1e50000ca194d7c9fbf1fdb0fcf1f"
   },
   "source": [
    "The news increases in volumes and urgency as price drops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "84e3ccc789ea7ad18dd2572798381a456dea24bd"
   },
   "outputs": [],
   "source": [
    "news_relevance = thisAssetNews_df.groupby('date')['relevance'].mean().reset_index()\n",
    "news_relevance = news_relevance.ewm(span=10).mean()\n",
    "news_relevance.plot(x='date',y='relevance')\n",
    "plt.title('Relevance vs time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "e20e2aff75083e4272e90ee207d68fb9ab6c846d"
   },
   "outputs": [],
   "source": [
    "news_sentiment = thisAssetNews_df.groupby('date')['sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive'].mean().reset_index()\n",
    "news_sentiment = news_sentiment.ewm(span=10).mean()\n",
    "news_sentiment.plot(x='date',y=['sentimentClass','sentimentNegative','sentimentNeutral','sentimentPositive'], alpha=0.8)\n",
    "plt.title('Sentiment vs time')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "beecbc8b43f8e1a01ae8152766b52eca9bcda795"
   },
   "source": [
    "Sentiments are mostly negative. Sentiment drops as price drops, which is expected.\n",
    "\n",
    "Now let's merge the news and market data and see their correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ce24e874acf859c5412cbc3752ba20986302b585"
   },
   "outputs": [],
   "source": [
    "# Merge news and market data. Only keep numeric columns\n",
    "thisAssetMark_number = thisAssetMark_df[columns_corr_market+['date']]\n",
    "thisAssetMark_number = thisAssetMark_number.groupby('date').mean().reset_index()\n",
    "thisAssetNews_number = thisAssetNews_df[columns_corr+['date']]\n",
    "thisAssetNews_number = thisAssetNews_number.groupby('date').mean().reset_index()\n",
    "thisAssetNews_number['news_volume'] = thisAssetNews_df.groupby('date')['sourceId'].count().reset_index()['sourceId']\n",
    "thisAssetMerge = pd.merge(thisAssetMark_number, thisAssetNews_number, how='left', on = 'date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0f7da060e5e8cc4a00f48ee539926abd2f590c4f"
   },
   "outputs": [],
   "source": [
    "columns_corr_merge = ['volume','open','close','returnsOpenPrevRaw1','returnsOpenPrevMktres1','returnsOpenPrevRaw10','returnsOpenPrevMktres10',\\\n",
    "                     'returnsOpenNextMktres10','news_volume','urgency','sentenceCount','relevance','sentimentClass',\\\n",
    "                     'noveltyCount24H','noveltyCount5D','volumeCounts24H','volumeCounts5D']\n",
    "colormap = plt.cm.RdBu\n",
    "plt.figure(figsize=(18,15))\n",
    "sns.heatmap(thisAssetMerge[columns_corr_merge].astype(float).corr(), linewidths=0.1, vmax=1.0, vmin=-1., square=True, cmap=colormap, linecolor='white', annot=True)\n",
    "plt.title('Pair-wise correlation market and news')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "5d72a56caffe12f78ba3792d545da37bf1d2ad16"
   },
   "source": [
    "This concludes the exploratory analysis. I will now proceed on data preprocessing and model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f2c72ef027202bf32dda3bca1b51d4feadd4076c"
   },
   "outputs": [],
   "source": [
    "del thisAssetMark_df\n",
    "del news_relevance\n",
    "del market_train_fill\n",
    "del news_train_df\n",
    "del news_rmv_outlier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "533230a1f3625768b4723b497ecb531e33b79505"
   },
   "source": [
    "<a id='preprocessing'></a>\n",
    "\n",
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f5326503524aad22fab6bf8492f89b2f8f050d48"
   },
   "outputs": [],
   "source": [
    "market_train_orig = market_train_orig.sort_values('time')\n",
    "news_train_orig = news_train_orig.sort_values('time')\n",
    "market_train_df = market_train_orig.copy()\n",
    "news_train_df = news_train_orig.copy()\n",
    "del market_train_orig\n",
    "del news_train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a07d4edb39df2b8a342cb10464089e98f3d6e866"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.loc[market_train_df['time'].dt.date>=datetime.date(2009,1,1)]\n",
    "news_train_df = news_train_df.loc[news_train_df['time'].dt.date>=datetime.date(2009,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3d96dd874095a29c55db87baa9696e40381fa9c9"
   },
   "source": [
    "### Market data\n",
    "* **Outliers - Open to close:** the difference between open price and close price cannot be too much difference (market would corrupt otherwise). We treat these outliers by clipping the close-to-open ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "68466b551f475190c8eefd24efc8609b32921ce9"
   },
   "outputs": [],
   "source": [
    "market_train_df['close_open_ratio'] = np.abs(market_train_df['close']/market_train_df['open'])\n",
    "threshold = 0.5\n",
    "print('In %i lines price increases by 50%% or more in a day' %(market_train_df['close_open_ratio']>=1.5).sum())\n",
    "print('In %i lines price decreases by 50%% or more in a day' %(market_train_df['close_open_ratio']<=0.5).sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "461e324fda87ccf91650719a5772b8250f0caca4"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.loc[market_train_df['close_open_ratio'] < 1.5]\n",
    "market_train_df = market_train_df.loc[market_train_df['close_open_ratio'] > 0.5]\n",
    "market_train_df = market_train_df.drop(columns=['close_open_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6a62f797db32122e8ce7d0a0aff226d658131a99"
   },
   "source": [
    "* **Fill nulls - Market values:** All null data comes from market adjusted columns. We fill them up with the raw values in the same row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ef83838cb75cbeb2e1bca6415ad6a65a6c1d11a1"
   },
   "outputs": [],
   "source": [
    "column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n",
    "for i in range(len(column_raw)):\n",
    "    market_train_df[column_market[i]] = market_train_df[column_market[i]].fillna(market_train_df[column_raw[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "c6666ba58af8b73e79c5ea29e8312578d05a81e9"
   },
   "source": [
    "* **Outliers-Returns:** Return should not exceed 50% or falls below 50%. If it does, it is either noise, or extreme data that will confuse our prediction later on. We remove these extreme data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3123f881db9827e614d29cb8ce40a688fe0abf76"
   },
   "outputs": [],
   "source": [
    "print('Removing outliers ...')\n",
    "column_return = column_market + column_raw + ['returnsOpenNextMktres10']\n",
    "orig_len = market_train_df.shape[0]\n",
    "for column in column_return:\n",
    "    market_train_df = market_train_df.loc[market_train_df[column]>=-2]\n",
    "    market_train_df = market_train_df.loc[market_train_df[column]<=2]\n",
    "new_len = market_train_df.shape[0]\n",
    "rmv_len = np.abs(orig_len-new_len)\n",
    "print('There were %i lines removed' %rmv_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "8416f18a33b93116654001c7749449ecdd4e1805"
   },
   "source": [
    "* **Remove strange data**: Here we remove data with unknown asset name or asset codes with strange behavior. For more details, see here: https://www.kaggle.com/nareyko/market-return-estimation-and-bad-data-detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a86c7c17afb7ab3de2a051c8aae005bc7c0d3c9c"
   },
   "outputs": [],
   "source": [
    "print('Removing strange data ...')\n",
    "orig_len = market_train_df.shape[0]\n",
    "market_train_df = market_train_df[~market_train_df['assetCode'].isin(['PGN.N','EBRYY.OB'])]\n",
    "#market_train_df = market_train_df[~market_train_df['assetName'].isin(['Unknown'])]\n",
    "new_len = market_train_df.shape[0]\n",
    "rmv_len = np.abs(orig_len-new_len)\n",
    "print('There were %i lines removed' %rmv_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "42420165b71e020f2962388d9c17513ca7304986"
   },
   "source": [
    "### News data\n",
    "* **Remove outliers**: apply a clip filter to reduce too extreme data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "f0b7ff853e590f8fd2d3d36a65f0953ec892a025"
   },
   "outputs": [],
   "source": [
    "# Function to remove outliers\n",
    "def remove_outliers(data_frame, column_list, low=0.02, high=0.98):\n",
    "    for column in column_list:\n",
    "        this_column = data_frame[column]\n",
    "        quant_df = this_column.quantile([low,high])\n",
    "        low_limit = quant_df[low]\n",
    "        high_limit = quant_df[high]\n",
    "        data_frame[column] = data_frame[column].clip(lower=low_limit, upper=high_limit)\n",
    "    return data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "659a7d37420c018f7349a9eea9b8537441243ad6"
   },
   "outputs": [],
   "source": [
    "# Remove outlier\n",
    "columns_outlier = ['takeSequence', 'bodySize', 'sentenceCount', 'wordCount', 'sentimentWordCount', 'firstMentionSentence','noveltyCount12H',\\\n",
    "                  'noveltyCount24H', 'noveltyCount3D', 'noveltyCount5D', 'noveltyCount7D', 'volumeCounts12H', 'volumeCounts24H',\\\n",
    "                  'volumeCounts3D','volumeCounts5D','volumeCounts7D']\n",
    "print('Clipping news outliers ...')\n",
    "news_train_df = remove_outliers(news_train_df, columns_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "47318e5ce19d29f9e0a0d8823c653ea6fad4bbd1"
   },
   "source": [
    "<a id='feature_engineering'></a>\n",
    "\n",
    "## Features engineering\n",
    "\n",
    "### Data processing function\n",
    "Here we make a function process both market and news data, then merge them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "7acf780c5abc2c5da49bbed1fd72de913b0bbd60"
   },
   "outputs": [],
   "source": [
    "asset_code_dict = {k: v for v, k in enumerate(market_train_df['assetCode'].unique())}\n",
    "drop_columns = [col for col in news_train_df.columns if col not in ['sourceTimestamp', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n",
    "               'sentenceCount', 'firstMentionSentence', 'relevance','firstCreated', 'assetCodes']]\n",
    "columns_news = ['firstCreated','relevance','sentimentClass','sentimentNegative','sentimentNeutral',\n",
    "               'sentimentPositive','noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodes','sourceTimestamp',\n",
    "               'assetName','audiences', 'urgency', 'takeSequence', 'bodySize', 'companyCount', \n",
    "               'sentenceCount', 'firstMentionSentence','time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "97faaaebe99bea04efc745693d38b966cf879c1e"
   },
   "outputs": [],
   "source": [
    "# Data processing function\n",
    "def data_prep(market_df,news_df):\n",
    "    market_df['date'] = market_df.time.dt.date\n",
    "    market_df['close_to_open'] = market_df['close'] / market_df['open']\n",
    "    market_df.drop(['time'], axis=1, inplace=True)\n",
    "    \n",
    "    news_df = news_df[columns_news]\n",
    "    news_df['sourceTimestamp']= news_df.sourceTimestamp.dt.hour\n",
    "    news_df['firstCreated'] = news_df.firstCreated.dt.date\n",
    "    news_df['assetCodesLen'] = news_df['assetCodes'].map(lambda x: len(eval(x)))\n",
    "    news_df['assetCodes'] = news_df['assetCodes'].map(lambda x: list(eval(x))[0])\n",
    "    news_df['asset_sentiment_count'] = news_df.groupby(['assetName', 'sentimentClass'])['time'].transform('count')\n",
    "    news_df['len_audiences'] = news_train_df['audiences'].map(lambda x: len(eval(x)))\n",
    "    kcol = ['firstCreated', 'assetCodes']\n",
    "    news_df = news_df.groupby(kcol, as_index=False).mean()\n",
    "    market_df = pd.merge(market_df, news_df, how='left', left_on=['date', 'assetCode'], \n",
    "                            right_on=['firstCreated', 'assetCodes'])\n",
    "    del news_df\n",
    "    market_df['assetCodeT'] = market_df['assetCode'].map(asset_code_dict)\n",
    "    market_df = market_df.drop(columns = ['firstCreated','assetCodes','assetName']).fillna(0) \n",
    "    return market_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "22c089c414023040b4b17a8517d21eb24f025833"
   },
   "outputs": [],
   "source": [
    "print('Merging data ...')\n",
    "market_train_df = data_prep(market_train_df, news_train_df)\n",
    "market_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "0d76f49957c4bdaec695ad7719bbfc76900ca863"
   },
   "source": [
    "<a id='data_selection'></a>\n",
    "\n",
    "### Data selection\n",
    "\n",
    "Looking at the statistics, most data behave homogeneously after 2009 (volume increase, price increase, etc.). However, before 2009, due to the burst of the housing bubble that leads to the financial crisis in 2008, the data behaves differently. So the question to make the right prediction for this problem is: **Will there be a financial crisis in the next 6 months?** If the answer is **Yes**, then we include data before 2009. If the answer is **No**, then we exclude them.\n",
    "\n",
    "In this notebook, I choose **No** as the answer and proceed from that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "9c43ae5ecdda576dbc908ee4afe7a8adf61ab10a"
   },
   "outputs": [],
   "source": [
    "market_train_df = market_train_df.loc[market_train_df['date']>=datetime.date(2009,1,1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "02734ce7cb4cb28bfb0b00c8d4bd2e53696d2a74"
   },
   "source": [
    "We then perform feature selection . Feature scaling is not needed since we plan to use lightgbm - a tree-based model, which do not require standardization.\n",
    "\n",
    "I tried using a regressor model, but a problem is that it gives close-to-0 values for most of prediction. Thus, I convert this problem into a classification problem: 0 for negative return and 1 for positive return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "bb2f32a2f104f05eeb0fe274587be846533dd059"
   },
   "outputs": [],
   "source": [
    "num_columns = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1', 'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', \n",
    "               'returnsClosePrevMktres10', 'returnsOpenPrevMktres10', 'close_to_open', 'sourceTimestamp', 'urgency', 'companyCount', 'takeSequence', 'bodySize', 'sentenceCount',\n",
    "               'relevance', 'sentimentClass', 'sentimentNegative', 'sentimentNeutral', 'sentimentPositive',\n",
    "               'noveltyCount24H','noveltyCount7D','volumeCounts24H','volumeCounts7D','assetCodesLen', 'asset_sentiment_count', 'len_audiences']\n",
    "cat_columns = ['assetCodeT']\n",
    "feature_columns = num_columns+cat_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "804b093a309dcc6f993f2158a15aaa40d646005f"
   },
   "outputs": [],
   "source": [
    "# Scaling of data\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "data_scaler = StandardScaler()\n",
    "#market_train_df[num_columns] = data_scaler.fit_transform(market_train_df[num_columns])\n",
    "#data_scaler = MinMaxScaler()\n",
    "market_train_df[num_columns] = data_scaler.fit_transform(market_train_df[num_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "cbf8c289fd90dcd07e1b51b5c418f4a9c518f865"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "market_train_df = market_train_df.reset_index()\n",
    "market_train_df = market_train_df.drop(columns='index')\n",
    "\n",
    "# Random train-test split\n",
    "train_indices, val_indices = train_test_split(market_train_df.index.values,test_size=0.1, random_state=92)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c9cd7c56481fbbd403506e7ca0e851006905463c"
   },
   "outputs": [],
   "source": [
    "# Extract X and Y\n",
    "def get_input(market_train, indices):\n",
    "    X = market_train.loc[indices, feature_columns].values\n",
    "    y = market_train.loc[indices,'returnsOpenNextMktres10'].map(lambda x: 0 if x<0 else 1).values\n",
    "    #y = market_train.loc[indices,'returnsOpenNextMktres10'].map(lambda x: convert_to_class(x)).values\n",
    "    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n",
    "    u = market_train.loc[indices, 'universe']\n",
    "    d = market_train.loc[indices, 'date']\n",
    "    return X,y,r,u,d\n",
    "\n",
    "# r, u and d are used to calculate the scoring metric\n",
    "X_train,y_train,r_train,u_train,d_train = get_input(market_train_df, train_indices)\n",
    "X_val,y_val,r_val,u_val,d_val = get_input(market_train_df, val_indices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f109a332b287d77ac6b2b6d586fa1cea9f8c6d54"
   },
   "source": [
    "<a id='building_model'></a>\n",
    "\n",
    "## Building model\n",
    "\n",
    "Here we use lightgbm classifier as our model\n",
    "\n",
    "### Parameters tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "11a7aabb57359d9e352ba60fe5b71c3b10c5e21c"
   },
   "outputs": [],
   "source": [
    "# Set up decay learning rate\n",
    "def learning_rate_power(current_round):\n",
    "    base_learning_rate = 0.19000424246380565\n",
    "    min_learning_rate = 0.01\n",
    "    lr = base_learning_rate * np.power(0.995,current_round)\n",
    "    return max(lr, min_learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b07f8ae59788b650dd3a580baf06e6825d802e66"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "\n",
    "tune_params = {'n_estimators': [200,500,1000,2500,5000],\n",
    "              'max_depth': sp_randint(4,12),\n",
    "              'colsample_bytree':sp_uniform(loc=0.8, scale=0.15),\n",
    "              'min_child_samples':sp_randint(60,120),\n",
    "              'subsample': sp_uniform(loc=0.75, scale=0.25),\n",
    "              'reg_lambda':[1e-3, 1e-2, 1e-1, 1]}\n",
    "\n",
    "fit_params = {'early_stopping_rounds':40,\n",
    "              'eval_metric': 'accuracy',\n",
    "              'eval_set': [(X_train, y_train), (X_val, y_val)],\n",
    "              'verbose': 20,\n",
    "              'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_power)]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3a01d9dd5c8aec006d56b1201fee4dc31e4f73d9"
   },
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(n_jobs=4, objective='binary',random_state=1)\n",
    "gs = RandomizedSearchCV(estimator=lgb_clf, \n",
    "                        param_distributions=tune_params, \n",
    "                        n_iter=40,\n",
    "                        scoring='f1',\n",
    "                        cv=5,\n",
    "                        refit=True,\n",
    "                        random_state=1,\n",
    "                        verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b6a3c0ba7e0c6e0b9f30dbf54e3347e70f9b7069"
   },
   "source": [
    "Running the parameters search will take another 3 hours, so we will straight away use the best parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8e8cf0735a82efde76dc8f5ea222ca1fc1ebf5ab"
   },
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(n_jobs=4,\n",
    "                             objective='multiclass',\n",
    "                            random_state=100)\n",
    "opt_params = {'n_estimators':500,\n",
    "              'boosting_type': 'dart',\n",
    "              'objective': 'binary',\n",
    "              'num_leaves':2452,\n",
    "              'min_child_samples':212,\n",
    "              'reg_lambda':0.01}\n",
    "lgb_clf.set_params(**opt_params)\n",
    "lgb_clf.fit(X_train, y_train,**fit_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "8645f1f4155e13a14bc9ef857ae7c2853421322c"
   },
   "outputs": [],
   "source": [
    "print('Training accuracy: ', accuracy_score(y_train, lgb_clf.predict(X_train)))\n",
    "print('Validation accuracy: ', accuracy_score(y_val, lgb_clf.predict(X_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f859fdfe7e91c630f16765190e00859666722cf1"
   },
   "source": [
    "<a id='visualinzg_result'></a>\n",
    "\n",
    "### Visualizing the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "79a41d0e7126c8c31fc1e71cd9f486147b264f37"
   },
   "outputs": [],
   "source": [
    "features_imp = pd.DataFrame()\n",
    "features_imp['features'] = list(feature_columns)[:]\n",
    "features_imp['importance'] = lgb_clf.feature_importances_\n",
    "features_imp = features_imp.sort_values(by='importance', ascending=False).reset_index()\n",
    "\n",
    "y_plot = -np.arange(15)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(y_plot, features_imp.loc[:14,'importance'].values)\n",
    "plt.yticks(y_plot,(features_imp.loc[:14,'features']))\n",
    "plt.xlabel('Feature importance')\n",
    "plt.title('Features importance')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1ab512bd883ea909f4901d54a7ead9bfe298eae6"
   },
   "outputs": [],
   "source": [
    "# Rescale confidence\n",
    "def rescale(data_in, data_ref):\n",
    "    scaler_ref =  StandardScaler()\n",
    "    scaler_ref.fit(data_ref.reshape(-1,1))\n",
    "    scaler_in = StandardScaler()\n",
    "    data_in = scaler_in.fit_transform(data_in.reshape(-1,1))\n",
    "    data_in = scaler_ref.inverse_transform(data_in)[:,0]\n",
    "    return data_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "3f02912a526528c04cb8d33e08bee6f7df5c16e9"
   },
   "outputs": [],
   "source": [
    "def confidence_out(y_pred):\n",
    "    confidence = np.zeros(y_pred.shape[0])\n",
    "    for i in range(len(confidence)):\n",
    "        if y_pred[i,:].argmax() != 1:\n",
    "            confidence[i] = y_pred[i,2]-y_pred[i,0]\n",
    "    return confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0039f3cbc0393c10633399116ab29dfead00bfa1"
   },
   "outputs": [],
   "source": [
    "y_pred_proba = lgb_clf.predict_proba(X_val)\n",
    "predicted_return = y_pred_proba[:,1] - y_pred_proba[:,0]\n",
    "#predicted_return = confidence_out(y_pred_proba)\n",
    "predicted_return = rescale(predicted_return, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "4594fd71ba2eb30775aa219bb4f32555bd2f1290"
   },
   "outputs": [],
   "source": [
    "# distribution of confidence that will be used as submission\n",
    "plt.hist(predicted_return, bins='auto', label='Predicted confidence')\n",
    "plt.hist(r_val, bins='auto',alpha=0.8, label='True market return')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0ead35ca766d8fed0d5de6dc798051ed0cda387b"
   },
   "outputs": [],
   "source": [
    "# calculation of actual metric that is used to calculate final score\n",
    "r_val = r_val.clip(-1,1) # get rid of outliers.\n",
    "x_t_i = predicted_return * r_val * u_val\n",
    "data = {'day' : d_val, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_valid = mean / std\n",
    "print('Validation score', score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3b5e2b2775991becf095e937cbdf5e6c0c9089da"
   },
   "source": [
    "<a id='voting_ensemble'></a>\n",
    "### Voting ensemble\n",
    "Now we construct an ensemble of multiple classifier and use soft voting to get the final result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "b3349437a5eddb48dc4aaed2b0303a744f7147a2"
   },
   "outputs": [],
   "source": [
    "# This code is inspired from this kernel: https://www.kaggle.com/skooch/lgbm-w-random-split-2\n",
    "clfs = []\n",
    "for i in range(20):\n",
    "    clf = lgb.LGBMClassifier(learning_rate=0.1, random_state=1200+i, silent=True,\n",
    "                             n_jobs=4, n_estimators=2500)\n",
    "    clf.set_params(**opt_params)\n",
    "    clfs.append(('lgbm%i'%i, clf))\n",
    "\n",
    "def split_data(X, y, test_percentage=0.2, seed=None):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_percentage)\n",
    "    return X_train, y_train, X_test, y_test \n",
    "\n",
    "def _parallel_fit_estimator(estimator, X, y, sample_weight=None, **fit_params):\n",
    "    \n",
    "    # randomly split the data so we have a test set for early stopping\n",
    "    X_train, y_train, X_test, y_test = split_data(X, y, seed=1992)\n",
    "    \n",
    "    # update the fit params with our new split\n",
    "    fit_params[\"eval_set\"] = [(X_train,y_train), (X_test,y_test)]\n",
    "    \n",
    "    # fit the estimator\n",
    "    if sample_weight is not None:\n",
    "        estimator.fit(X_train, y_train, sample_weight=sample_weight, **fit_params)\n",
    "    else:\n",
    "        estimator.fit(X_train, y_train, **fit_params)\n",
    "    return estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "0e0f7a3f920055675759fb243c9afda8baa57658"
   },
   "outputs": [],
   "source": [
    "class VotingClassifierLGBM(VotingClassifier):\n",
    "    '''\n",
    "    This implements the fit method of the VotingClassifier propagating fit_params\n",
    "    '''\n",
    "    def fit(self, X, y, sample_weight=None, **fit_params):\n",
    "        \n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\n",
    "                                      ' classification is not supported.')\n",
    "\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % self.voting)\n",
    "\n",
    "        if self.estimators is None or len(self.estimators) == 0:\n",
    "            raise AttributeError('Invalid `estimators` attribute, `estimators`'\n",
    "                                 ' should be a list of (string, estimator)'\n",
    "                                 ' tuples')\n",
    "\n",
    "        if (self.weights is not None and\n",
    "                len(self.weights) != len(self.estimators)):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d estimators'\n",
    "                             % (len(self.weights), len(self.estimators)))\n",
    "\n",
    "        if sample_weight is not None:\n",
    "            for name, step in self.estimators:\n",
    "                if not has_fit_parameter(step, 'sample_weight'):\n",
    "                    raise ValueError('Underlying estimator \\'%s\\' does not'\n",
    "                                     ' support sample weights.' % name)\n",
    "        names, clfs = zip(*self.estimators)\n",
    "        self._validate_names(names)\n",
    "\n",
    "        n_isnone = np.sum([clf is None for _, clf in self.estimators])\n",
    "        if n_isnone == len(self.estimators):\n",
    "            raise ValueError('All estimators are None. At least one is '\n",
    "                             'required to be a classifier!')\n",
    "\n",
    "        self.le_ = LabelEncoder().fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.estimators_ = []\n",
    "\n",
    "        transformed_y = self.le_.transform(y)\n",
    "\n",
    "        self.estimators_ = Parallel(n_jobs=self.n_jobs)(\n",
    "                delayed(_parallel_fit_estimator)(clone(clf), X, transformed_y,\n",
    "                                                 sample_weight=sample_weight, **fit_params)\n",
    "                for clf in clfs if clf is not None)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "4250806eef11526dea2c75ba6c7646151b5ed89a"
   },
   "outputs": [],
   "source": [
    "vc = VotingClassifierLGBM(clfs, voting='soft')\n",
    "vc.fit(X_train, y_train, **fit_params)\n",
    "filename = 'VotingClassifierLGBM.sav'\n",
    "pickle.dump(vc, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "937291f6fb46376eb3c2ba9b6af22747712d3de4"
   },
   "outputs": [],
   "source": [
    "vc = pickle.load(open(filename, 'rb'))\n",
    "vc.voting = 'soft'\n",
    "predicted_class = vc.predict(X_val)\n",
    "predicted_return = vc.predict_proba(X_val)\n",
    "#predicted_return = confidence_out(predicted_return)\n",
    "predicted_return = vc.predict_proba(X_val)[:,1]*2-1\n",
    "predicted_return = rescale(predicted_return, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "acb35559e7d2397d6171a868ac4bbba0193f52fb"
   },
   "outputs": [],
   "source": [
    "plt.hist(predicted_class, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "1b1d3be6e63555feb2d3e7534171e5a5f7e1023c"
   },
   "outputs": [],
   "source": [
    "vc.voting = 'soft'\n",
    "global_accuracy_soft = accuracy_score(y_val, predicted_class)\n",
    "global_f1_soft = f1_score(y_val, predicted_class)\n",
    "print('Accuracy score clfs: %f' % global_accuracy_soft)\n",
    "print('F1 score clfs: %f' % global_f1_soft)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "6900a1f6173704a54c8470d3b6c46e539170232d"
   },
   "outputs": [],
   "source": [
    "# distribution of confidence that will be used as submission\n",
    "plt.hist(predicted_return, bins='auto', label='Prediciton')\n",
    "plt.hist(r_val, bins='auto',alpha=0.8, label='True data')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlim(-1,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "ae65c3ff019cc6450beb18b0770d1200ba7faf77"
   },
   "outputs": [],
   "source": [
    "# calculation of actual metric that is used to calculate final score\n",
    "r_val = r_val.clip(-1,1) # get rid of outliers. Where do they come from??\n",
    "x_t_i = predicted_return * r_val * u_val\n",
    "data = {'day' : d_val, 'x_t_i' : x_t_i}\n",
    "df = pd.DataFrame(data)\n",
    "x_t = df.groupby('day').sum().values.flatten()\n",
    "mean = np.mean(x_t)\n",
    "std = np.std(x_t)\n",
    "score_valid = mean / std\n",
    "print('Validation score', score_valid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e5ce97723159d0633f7a91263a19267a9aee709d"
   },
   "source": [
    "<a id='making_submission'></a>\n",
    "## Making submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-input": true,
    "_uuid": "74f202932f11e4fef607a0d2fb1b88f10ee5b500"
   },
   "outputs": [],
   "source": [
    "days = env.get_prediction_days()\n",
    "n_days = 0\n",
    "prep_time = 0\n",
    "prediction_time = 0\n",
    "packaging_time = 0\n",
    "for (market_obs_df, news_obs_df, predictions_template_df) in days:\n",
    "    n_days +=1\n",
    "    if n_days % 50 == 0:\n",
    "        print(n_days,end=' ')\n",
    "\n",
    "    t = time.time()\n",
    "    column_market = ['returnsClosePrevMktres1','returnsOpenPrevMktres1','returnsClosePrevMktres10', 'returnsOpenPrevMktres10']\n",
    "    column_raw = ['returnsClosePrevRaw1', 'returnsOpenPrevRaw1','returnsClosePrevRaw10', 'returnsOpenPrevRaw10']\n",
    "    market_obs_df['close_open_ratio'] = np.abs(market_obs_df['close']/market_obs_df['open'])\n",
    "    for i in range(len(column_raw)):\n",
    "        market_obs_df[column_market[i]] = market_obs_df[column_market[i]].fillna(market_obs_df[column_raw[i]])\n",
    "\n",
    "    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(predictions_template_df.assetCode)]\n",
    "    market_obs_df = market_obs_df[market_obs_df.assetCode.isin(asset_code_dict.keys())]\n",
    "    market_obs = data_prep(market_obs_df, news_obs_df)\n",
    "    market_obs[num_columns] = data_scaler.transform(market_obs[num_columns])\n",
    "    X_live = market_obs[feature_columns].values\n",
    "    prep_time += time.time() - t\n",
    "\n",
    "    t = time.time()\n",
    "    lp = vc.predict_proba(X_live)\n",
    "    prediction_time += time.time() -t\n",
    "\n",
    "    t = time.time()\n",
    "    confidence = lp[:,1] - lp[:,0]\n",
    "    #confidence = confidence_out(lp)\n",
    "    confidence = rescale(confidence, r_train)\n",
    "    preds = pd.DataFrame({'assetCode':market_obs['assetCode'],'confidence':confidence})\n",
    "    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n",
    "    env.predict(predictions_template_df)\n",
    "    packaging_time += time.time() - t\n",
    "\n",
    "env.write_submission_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "01580ec6b5f8d54464414ee5ce477f083936ec41"
   },
   "outputs": [],
   "source": [
    "plt.hist(confidence, bins='auto')\n",
    "plt.title(\"predicted confidence\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
